{
  "paragraphs": [
    {
      "text": "%pyspark\n\nimport emcee\n\n\n\n\n\n\n",
      "user": "nch",
      "dateUpdated": "2022-05-30 14:29:36.185",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 3: import emcee\nTraceback (most recent call last):\n  File \"/tmp/1653407474337-0/zeppelin_python.py\", line 158, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 3, in \u003cmodule\u003e\nModuleNotFoundError: No module named \u0027emcee\u0027\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1653919455089_678269209",
      "id": "paragraph_1653919455089_678269209",
      "dateCreated": "2022-05-30 14:04:15.089",
      "dateStarted": "2022-05-30 14:29:36.188",
      "dateFinished": "2022-05-30 14:29:36.195",
      "status": "ERROR"
    },
    {
      "text": "%pyspark\n\nfrom gaiadmpsetup import gaiadmpstore \n",
      "user": "nch",
      "dateUpdated": "2022-06-07 09:19:54.095",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1653919461733_941781818",
      "id": "paragraph_1653919461733_941781818",
      "dateCreated": "2022-05-30 14:04:21.733",
      "dateStarted": "2022-06-07 09:19:23.195",
      "dateFinished": "2022-06-07 09:19:23.204",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\nhelp(gaiadmpstore)\n",
      "user": "nch",
      "dateUpdated": "2022-06-07 09:20:57.903",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Help on module gaiadmpsetup.gaiadmpstore in gaiadmpsetup:\n\nNAME\n    gaiadmpsetup.gaiadmpstore\n\nFUNCTIONS\n    cast_all_arrays(data_frame: pyspark.sql.dataframe.DataFrame, data_structure: pyspark.sql.types.StructType)\n        Given an interim data frame read from csv and containing arrays in\n        plain text string representation, cycles over the schema transforming\n        all strings associated with arrays into the required primitive type.\n        \n        Parameters:\n        -----------\n        data_frame : DataFrame()\n            The PySpark data frame instance to be operated on\n        data_structure : StructType()\n            The PySpark data structure containing the required schema definition\n    \n    cast_to_array(data_frame: pyspark.sql.dataframe.DataFrame, column_name: str, data_type: pyspark.sql.types.DataType)\n        Casts the specified string column in the given data frame into an\n        array with the specified data type. Assumes the string column contains\n        comma-separated values in plain text delimited by braces (which are\n        ignored). The array column is appended to the existing column set while \n        the original string column is removed. The resulting data frame will\n        contain an array column with the same name as the original string\n        data column.\n        \n        Parameters:\n        -----------\n        data_frame : DataFrame()\n            The PySpark data frame instance to be operated on\n        column_name : str\n            The column name that contains the array data as a plain text string of \n            comma-separated values\n        data_type : DataType()\n            The PySpark data structure data type which should be ArrayType(SomeType())\n            \n        Returns:\n        --------\n        a new data frame containing the requested modification\n    \n    create_interim_schema_for_csv(schema_structure)\n        Takes a schema StructType() and substitutes all array types as a string in order\n        to create a schema against which csv files can be read into an interim data frame\n        prior to conversion of the comma-separated string of numerical values into an array\n        of the appropriate numeric type.\n        \n        Parameters\n        ----------\n        schema_structure : StructType\n            the table schema containing array types \n            \n        Returns: StructType\n        -------------------\n        An edited version of the given schema with all ArrayType changed to StringType\n    \n    reattachParquetFileResourceToSparkContext(table_name, file_path, schema_structures, cluster_key\u003d\u0027source_id\u0027, sort_key\u003d\u0027source_id\u0027, buckets\u003d2048)\n        Creates a Spark (in-memory) meta-record for the table resource specified for querying\n        through the PySpark SQL API.\n        \n        Default assumption is that the table contains the Gaia source_id attribute and that the files have\n        been previously partitioned, bucketed and sorted on this field in parquet format\n        - see function saveToBinnedParquet().  If the table name specified already exists in the\n        catalogue IT WILL BE REMOVED (but the underlying data, assumed external, will remain).\n        \n        Parameters\n        ----------\n        table_name : str\n                The table name to be used as the identifier in SQL queries etc.\n        file_path : str\n                The full disk file system path name to the folder containing the parquet file set.\n        schema_structures : StructType\n                One or more schema structures expressed as a StructType object containing a list of\n                StructField(field_name : str, type : data_type : DataType(), nullable : boolean)\n        cluster_key : str (optional)\n            The clustering key (\u003d bucketing and sort key) in the partitioned data set on disk. \n            Default is Gaia catalogue source UID (\u003d source_id).\n        sort_key : str (optional)\n            The sorting key within buckets in the partitioned data set on disk. \n            Default is Gaia catalogue source UID (\u003d source_id).\n        buckets : int (optional)\n            Number of buckets into which the data is organised.\n    \n    reorder_columns(data_frame: pyspark.sql.dataframe.DataFrame, data_structure: pyspark.sql.types.StructType)\n        Reorder the columns according to the Gaia archive public schema and so that\n        the parquet files can be re-attached against that standard schema.\n        \n        Parameters:\n        -----------\n        data_frame : DataFrame()\n            The PySpark data frame instance to be operated on\n        data_structure : StructType()\n            The PySpark data structure containing the required schema definition\n    \n    saveToBinnedParquet(df, outputParquetPath, name, mode\u003d\u0027error\u0027, buckets\u003d2048, bucket_and_sort_key\u003d\u0027source_id\u0027)\n        Save a data frame to a set of bucketed parquet files, repartitioning beforehand and sorting\n        (by default on DPAC Gaia source UID) within the buckets.\n        \n        Parameters\n        ----------\n        df : DataFrame\n            (mandatory) the data frame to be written to the persistent file store\n        outputParquetPath : str\n            (mandatory) the absolute path name of the folder to contain the files\n        mode : str\n            (optional) the mode for the underlying DataFrame write method (default : error, i.e. no silent failures)\n        buckets : int\n            (optional) the number of buckets (if in doubt leave as the default)\n        bucket_and_sort_key : str\n            (optional) the bucketing and sorting key for the keyed/clustered data (default source_id; you must specify\n            an alternative if the data frame does not have this column present)\n\nDATA\n    NUM_BUCKETS \u003d 2048\n    data_store \u003d \u0027file:////data/gaia/\u0027\n    default_key \u003d \u0027source_id\u0027\n    spark \u003d \u003cpyspark.sql.session.SparkSession object\u003e\n\nFILE\n    /usr/local/lib/python3.7/site-packages/gaiadmpsetup/gaiadmpstore.py\n\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654593563193_1892236665",
      "id": "paragraph_1654593563193_1892236665",
      "dateCreated": "2022-06-07 09:19:23.194",
      "dateStarted": "2022-06-07 09:20:57.904",
      "dateFinished": "2022-06-07 09:20:57.925",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n",
      "user": "nch",
      "dateUpdated": "2022-06-07 09:20:08.831",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654593608831_697397146",
      "id": "paragraph_1654593608831_697397146",
      "dateCreated": "2022-06-07 09:20:08.831",
      "status": "READY"
    }
  ],
  "name": "library testing",
  "id": "2H7AT874N",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}